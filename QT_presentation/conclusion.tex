\begin{frame}{Conclusion and future steps}
    \begin{itemize}
        \item Both neural network separations show very good separation for an early stage of optimisation
        \vspace{0.2cm}
        \item For the hadhad channel the separation is outstanding but the stability is bad. This means the agreement between training and validation is often bad
        or has regions that are not understood.
        \vspace{0.2cm}
        \item In both cases the variables have the most significant impact on the result. Different sets of features need to be explored to find better separations.
        \vspace{0.2cm}
        \item A decorrelation and ranking would be desireable
        \vspace{0.2cm}
        \item In the case of lephad several variables have to be added.
        \vspace{0.2cm}
        \item The handling of negative weights is an ongoing problem that needs to be investigated to see if a conclusion can be reached for neural networks.
    \end{itemize}
\end{frame}

\begin{frame}
    \begin{enumerate}
        \item Understand impact of negative weights on variable shape
        \item Investigate variable shapes for different treatments
        \item Understand impact of negative weights in ML algorithms
        \item Test different approaches for negative weight handling and investigate the model stability.
    \end{enumerate}
\end{frame}